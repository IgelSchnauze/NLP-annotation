{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "legislative-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "from string import punctuation\n",
    "\n",
    "from method_Luhn import choose_most_freq_words, count_range_sentences, choose_sent_for_summ\n",
    "from rating import count_rating\n",
    "from with_mystem.preprocessing_bag_words import unpack, preprocess, bag_words_vector\n",
    "\n",
    "# PREPROCESS\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "essential-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(file):\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        json_list = list(f)\n",
    "\n",
    "    all_news_text_summ = []\n",
    "    for i, str in enumerate(json_list):\n",
    "        if i > 1:\n",
    "            break\n",
    "        one_news = json.loads(str)\n",
    "        del one_news['url']\n",
    "        del one_news['title']\n",
    "        del one_news['date']\n",
    "        all_news_text_summ.append(one_news)\n",
    "    return all_news_text_summ\n",
    "\n",
    "\n",
    "def preprocess(text, punctuation_pack):\n",
    "    mystem = Mystem()\n",
    "    rus_stopwords = stopwords.words(\"russian\")\n",
    "    punctuation_pack = punctuation_pack + '—' + '«' + '»' + '», —'\n",
    "\n",
    "    text_sent_with_stop = ' # '.join(nltk.sent_tokenize(text))  # between sentences not ., but #\n",
    "    print(text_sent_with_stop)\n",
    "    all_sent_token_lemmas = mystem.lemmatize(text_sent_with_stop.lower())\n",
    "    all_sent_token_lemmas = [lemma for lemma in all_sent_token_lemmas if lemma != ' ' \\\n",
    "                             and (lemma.strip() not in punctuation_pack or lemma.strip() == '#') \\\n",
    "                             and lemma not in rus_stopwords]\n",
    "\n",
    "    sent_token_lemmas = spliting_list(all_sent_token_lemmas, '#')\n",
    "    return sent_token_lemmas, nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "neither-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spliting_list(list, delimiter):\n",
    "    splitted = [[]]\n",
    "    for lemma in list:\n",
    "        if lemma == delimiter:\n",
    "            splitted.append([])\n",
    "        else:\n",
    "            splitted[-1].append(lemma)\n",
    "    return splitted\n",
    "\n",
    "\n",
    "def bag_words_vector(all_sent_list):\n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    all_sent_str = [\" \".join(one_sent_list) for one_sent_list in all_sent_list]\n",
    "    bag_of_words = vectorizer.fit_transform(all_sent_str)\n",
    "    return bag_of_words\n",
    "\n",
    "def tfidf_bag_words(all_sent_list):\n",
    "    tf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    all_sent_str = [\" \".join(one_sent_list) for one_sent_list in all_sent_list]\n",
    "    bag_of_words = tf_vectorizer.fit_transform(all_sent_str)\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "renewable-memory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "«По итогам 2011 года чистый отток может составить примерно $80 млрд, в следующем году — около $20 млрд. # При этом мы ожидаем, что со второго полугодия 2012 года начнется приток капитала», — заявил «Интерфаксу» замминистра экономического развития Андрей Клепач. # Официальные прогнозы по выводу капитала из России становятся все пессимистичными: еще летом власти полагали, что из страны уйдет не более $35 млрд, в сентябре Минэкономразвития назвал цифру $50 млрд, в начале ноября Центробанк пересмотрел оценку до $70 млрд. # Очередное изменение прогноза было ожидаемо: по расчетам Центробанка , за январь — октябрь чистый отток капитала достиг $64 млрд, причем в последние месяцы он ускорился: в сентябре он составил $14 млрд, в октябре — $13 млрд против среднего ежемесячного оттока в $6—8 млрд в первом полугодии. # «После октябрьских данных Минэкономразвития вынуждено было изменить оценку, настаивать на $70 млрд означало ожидать серьезного замедления оттока капитала на непонятно каких причинах», — говорит главный экономист BNP Paribas Юлия Цепляева. # «В последние два месяца отток капитала ускорится, на декабрь приходится значительная часть выплат по внешним долгам, что приводит к усилению оттока, особенно если они не рефинансируются новыми кредитами», — соглашается главный экономист ФК «Открытие» Владимир Тихомиров. # Прогнозируемый Минэкономразвития отток капитала — один из самых высоких за последние 20 лет. # Больше ушло лишь в 2008 году на фоне разрастания финансового кризиса и российско-грузинской войны — $133,7 млрд. # В кризисный 2009 год из России утекло $56,1 млрд. # Главный фактор ускорения оттока капитала в 2011 году — нестабильность на внешних финансовых рынках и рост опасений относительно второй волны рецессии. # «Это реакция на неуверенность, которую генерирует Европа с долговыми проблемами. # В случае новой волны глобальной турбулентности Россия — одна из самых уязвимых стран», — говорит Цепляева. # Еще одна причина — ослабление рубля. # «Привлекательность вложений снижается на фоне того, что рубль перестал укрепляться, а ставки по депозитам достаточно низкие. # В результате экспортеры не полностью возвращают экспортную выручку», — говорит Тихомиров. # Внутри страны эксперты не видят особых причин для бегства капитала. # «Ситуация выглядит достаточно позитивно, очень хорошие макроэкономические результаты за год, особенно на фоне других стран. # С политической точки зрения все достаточно понятно и предсказуемо, итог выборов очевиден», — говорит экономист ИК «Тройка Диалог» Антон Струченевский. # Тем не менее политический фактор играет роль. # «Бизнесу важно не только, кто будет президентом, он ждет ясности с перестановками в правительстве. # В наших условиях административный ресурс важнее всего для успешности бизнеса», — говорит Цепляева, добавляя, что отток капитала продолжится до завершения президентских выборов.\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "all_news_text_summ = unpack('C:/Users/Acer/PycharmProjects/ML_NLP_course3/gazeta_jsonl/gazeta_train.jsonl')\n",
    "text = all_news_text_summ[0]['text']\n",
    "text_sent_token_lemmas, text_sent_token = preprocess(text, punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-analysis",
   "metadata": {},
   "source": [
    "bag_of_words = bag_words_vector(text_sent_token_lemmas)\n",
    "tf_bag_of_words = tfidf_bag_words(text_sent_token_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "subtle-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD LUHN\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def choose_most_freq_words(bag_of_words):\n",
    "    freq = bag_of_words.toarray().sum(axis=0)  # count frequency every word\n",
    "    sort_freq_index = np.argsort(freq)\n",
    "    top_words_index = sort_freq_index[-int(len(freq) / 10):]  # 10% most freq words\n",
    "    # print(freq[sort_freq_index[-int(len(freq)/10):]])\n",
    "    return top_words_index\n",
    "\n",
    "\n",
    "def count_range_sentences(bag_of_words, top_words_index):\n",
    "    sent_range_list = []\n",
    "    for i, sent in enumerate(bag_of_words.toarray()):  # take word-index string\n",
    "        use_words_index = np.nonzero(sent)[0]  # [0], because take array from info-tuple\n",
    "        words_count = len(use_words_index)\n",
    "        top_words_count = len(np.intersect1d(top_words_index, use_words_index))\n",
    "\n",
    "        sent_range = (top_words_count ** 2) / words_count\n",
    "        sent_range_list.append(sent_range)\n",
    "    return sent_range_list\n",
    "\n",
    "\n",
    "def choose_sent_for_summ(sent_range_list, text_sent_token):\n",
    "    sort_sent_range = np.argsort(sent_range_list)\n",
    "    sent_for_summ_index = np.sort(\n",
    "        sort_sent_range[-int(len(sent_range_list) * 0.1):])  # 10% most important sentences in right order\n",
    "\n",
    "    # summarize_text = \" \".join([sent for i, sent in enumerate(doc.sents) if any(i==sent_for_summ_index)])\n",
    "    total_summarize = [sent for i, sent in enumerate(text_sent_token) if any(i == sent_for_summ_index)]\n",
    "    # print(summarize)\n",
    "    return sent_for_summ_index, total_summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "attended-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "\n",
    "top_words_index = choose_most_freq_words(bag_of_words)\n",
    "sent_range_list = count_range_sentences(bag_of_words, top_words_index)\n",
    "sent_for_summ_index, total_summarize = choose_sent_for_summ(sent_range_list, text_sent_token)\n",
    "\n",
    "top_words_index_tf = choose_most_freq_words(tf_bag_of_words)\n",
    "sent_range_list_tf = count_range_sentences(tf_bag_of_words, top_words_index_tf)\n",
    "sent_for_summ_index_tf, total_summarize_tf = choose_sent_for_summ(sent_range_list_tf, text_sent_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "specialized-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "\n",
    "# compare generated with model summarize (count rating)\n",
    "all_summ_sent_lemmas, nlp_summ_text = preprocess(all_news_text_summ[0]['summary'], punctuation)\n",
    "count_model_summ_sent = len(all_summ_sent_lemmas)\n",
    "# add generated summary, to make bag\n",
    "for i, sent_list in enumerate(text_sent_token_lemmas):\n",
    "    if any(i == sent_for_summ_index):\n",
    "        all_summ_sent_lemmas.append(sent_list)\n",
    "\n",
    "bag_of_words_summ = bag_words_vector(all_summ_sent_lemmas)\n",
    "bag_of_words_summ_tf = tfidf_bag_words(all_summ_sent_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sustained-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rating(bag_of_words_summ, count_model_summ_sent):\n",
    "    # search center vector (vectors = sentence, n-space = number of words in bag)\n",
    "    bag_of_words_summ = bag_of_words_summ.toarray()\n",
    "    center_gener = np.sum(bag_of_words_summ[count_model_summ_sent:], axis=0)\n",
    "    center_model = np.sum(bag_of_words_summ[:count_model_summ_sent], axis=0)\n",
    "    print(center_gener, center_model)\n",
    "\n",
    "    # count angle between center vectors\n",
    "    center_gener_unit = center_gener / np.linalg.norm(center_gener)\n",
    "    center_model_unit = center_model / np.linalg.norm(center_model)\n",
    "    rating = np.arccos(np.clip(np.dot(center_gener_unit, center_model_unit), -1.0, 1.0))\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "empirical-tribute",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 2 0 0 0 0 2 1 0 1 1 0\n",
      " 1 1 1 2 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0] [0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 2 1 1 1 1 1 2 1 0 0 1\n",
      " 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1]\n",
      "[0.378425   0.         0.17837068 0.         0.21752142 0.21752142\n",
      " 0.         0.         0.         0.         0.21752142 0.\n",
      " 0.378425   0.21752142 0.21752142 0.31031392 0.         0.21752142\n",
      " 0.         0.         0.         0.         0.         0.21752142\n",
      " 0.         0.21752142 0.35355067 0.         0.         0.\n",
      " 0.         0.41258111 0.15059278 0.         0.21752142 0.21752142\n",
      " 0.         0.21752142 0.21752142 0.21752142 0.41258111 0.17837068\n",
      " 0.         0.         0.         0.         0.378425   0.\n",
      " 0.         0.21752142 0.         0.378425   0.         0.\n",
      " 0.378425   0.21752142 0.         0.         0.         0.\n",
      " 0.         0.         0.21752142 0.21752142 0.21752142 0.        ] [0.         0.38833805 0.24924397 0.38833805 0.         0.\n",
      " 0.31080556 0.30395076 0.27916789 0.31080556 0.         0.27916789\n",
      " 0.         0.         0.         0.31844276 0.30395076 0.\n",
      " 0.31080556 0.27916789 0.27916789 0.27916789 0.27916789 0.\n",
      " 0.27916789 0.         0.35000715 0.27916789 0.27916789 0.30395076\n",
      " 0.30395076 0.26885126 0.47928014 0.30395076 0.         0.\n",
      " 0.31080556 0.         0.         0.         0.1932714  0.24924397\n",
      " 0.30395076 0.31080556 0.31080556 0.30395076 0.         0.31080556\n",
      " 0.31080556 0.         0.30395076 0.         0.27916789 0.38833805\n",
      " 0.         0.         0.27916789 0.38833805 0.27916789 0.38833805\n",
      " 0.31080556 0.30395076 0.         0.         0.         0.31080556]\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "score = count_rating(bag_of_words_summ, count_model_summ_sent)\n",
    "score_tf = count_rating(bag_of_words_summ_tf, count_model_summ_sent)\n",
    "# print(score, score_tf) # почему-то для tf угол ближе к 90*, что плохо"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
