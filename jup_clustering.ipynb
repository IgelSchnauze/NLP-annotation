{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aggregate-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "standard-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_out_train.pickle', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "# with open('Y_out_train.pickle', 'rb') as f:\n",
    "#     Y_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "medical-corruption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=3000, max_iter=10, n_clusters=1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10 # сколько кластеров????\n",
    "model_km = MiniBatchKMeans(n_clusters=k, max_iter=10, batch_size = 3000)\n",
    "model_km.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "double-prevention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5506752731280473\n"
     ]
    }
   ],
   "source": [
    "print(model_km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "patient-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_out_test.pickle', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "with open('count_sent_in_text_X_test.pickle', 'rb') as f:\n",
    "    count_sent_X_test = pickle.load(f)\n",
    "with open('count_sent_in_text_Y_test.pickle', 'rb') as f:\n",
    "    count_sent_Y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ongoing-potential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592f8147a84d4cd690d9f7696677e1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 14] 25\n",
      "[33  6  4 15] 19101\n",
      "[19  6  3] 38022\n",
      "[23 36  5] 56568\n",
      "[17  7 16] 75597\n",
      "[39 28 34 11] 94343\n",
      "[ 9  1 17] 112855\n",
      "[ 0 22  5] 131907\n",
      "[14 28 29 24] 150613\n",
      "[15 25 27] 169600\n",
      "[7 9] 188696\n",
      "[9] 207704\n"
     ]
    }
   ],
   "source": [
    "index_sent_for_summ = []\n",
    "new_text_start = 0\n",
    "for text_ind in tqdm(range(len(count_sent_X_test))):\n",
    "    text_end = new_text_start + count_sent_X_test[text_ind]\n",
    "    text = X_test[new_text_start : text_end]\n",
    "    cluster_count = count_sent_Y_test[text_ind]\n",
    "    \n",
    "    model_km = KMeans(n_clusters = cluster_count)\n",
    "    cluster_field = model_km.fit(text)\n",
    "#     predict_cluster = model_km.predict(text)\n",
    "    distance_to_cluster = model_km.transform(text)\n",
    "    \n",
    "    sent_ind_close_centroids = np.argmin(distance_to_cluster, axis=0)\n",
    "    index_sent_for_summ.append(sent_ind_close_centroids)\n",
    "    new_text_start = text_end\n",
    "    if text_ind%500==0:\n",
    "        print(sent_ind_close_centroids, new_text_start)\n",
    "#     print(sent_ind_close_centroids)\n",
    "#     print(model_km.cluster_centers_[1][8000:9000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "spectacular-genius",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-2ddc1f4f4d70>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  pickle.dump(np.array(index_sent_for_summ), f)\n"
     ]
    }
   ],
   "source": [
    "with open('cluster_index_sent_for_summ_X_test.pickle', 'wb') as f:\n",
    "    pickle.dump(np.array(index_sent_for_summ), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
