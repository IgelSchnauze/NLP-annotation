{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to(pickle_file, data):\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "# распаковка файла и выбор нужных данных оттуда\n",
    "def unpack(file):\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        json_list = list(f)\n",
    "\n",
    "    all_news_text_X = []\n",
    "    all_news_summ_Y = []\n",
    "    for i, str in enumerate(json_list):\n",
    "        one_news = json.loads(str)\n",
    "        del one_news['url']\n",
    "        del one_news['title']\n",
    "        del one_news['date']\n",
    "        all_news_text_X.append(one_news['text'])\n",
    "        all_news_summ_Y.append(one_news['summary'])\n",
    "    return all_news_text_X, all_news_summ_Y\n",
    "\n",
    "# разбиение листа внутри на несколько листов по заданному разделителю\n",
    "def spliting_list(list_, delimiter):\n",
    "    splitted = [[]]\n",
    "    for lemma in list_:\n",
    "        if lemma == delimiter:\n",
    "            splitted.append([])\n",
    "        else:\n",
    "            splitted[-1].append(lemma)\n",
    "    return splitted\n",
    "\n",
    "# предобработка (нормализация, лемматизация)\n",
    "def preprocess(text_str, punctuation_re):\n",
    "    mystem = Mystem()\n",
    "    rus_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "    text_lemmas = mystem.lemmatize(text_str.lower())\n",
    "    text_lemmas = [punctuation_re.sub('', lemma) for lemma in text_lemmas ]  # удаляю все кроме букв и цифр\n",
    "    text_lemmas = [lemma for lemma in text_lemmas if lemma.strip() != '' and lemma not in rus_stopwords]\n",
    "\n",
    "    return text_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF векторизация с созданием базового мешка слов\n",
    "def tfidf_bag_words_make_vocab(all_sent_list):\n",
    "    tf_vectorizer = TfidfVectorizer(max_features=50000, max_df=0.85)  # слово встречается > чем в 85% -> не берем\n",
    "    all_sent_str = [\" \".join(one_sent_list) for one_sent_list in all_sent_list]  \n",
    "    bag_of_words = tf_vectorizer.fit_transform(all_sent_str)\n",
    "    vocabulary = tf_vectorizer.get_feature_names()\n",
    "    return bag_of_words, vocabulary\n",
    "\n",
    "# TF-IDF векторизация по базовому мешку слов\n",
    "def tfidf_bag_words_with_vocab(all_sent_list, train_vocabulary):\n",
    "    tf_vectorizer = TfidfVectorizer(vocabulary = train_vocabulary)\n",
    "    all_sent_str = [\" \".join(one_sent_list) for one_sent_list in all_sent_list] \n",
    "    bag_of_words = tf_vectorizer.fit_transform(all_sent_str)\n",
    "    return bag_of_words\n",
    "\n",
    "# сравнение двух векторов-предложений\n",
    "def score_compare_sent(vec_X, vec_Y):\n",
    "    # угол между векторами\n",
    "    vec_X_unit = (vec_X / np.linalg.norm(vec_X)) if np.linalg.norm(vec_X) != 0 else np.zeros(vec_X.shape)\n",
    "    vec_Y_unit = (vec_Y / np.linalg.norm(vec_Y)) if np.linalg.norm(vec_Y) != 0 else np.zeros(vec_Y.shape)\n",
    "    angles = np.arccos(np.clip(np.dot(vec_X_unit, vec_Y_unit), -1.0, 1.0))\n",
    "    # привожу [0 - хорошо, pi/2 - плохо] к [0 - плохо, 1 - хорошо]\n",
    "    score_01 = 1 - (angles / (np.pi/2))\n",
    "    return score_01\n",
    "\n",
    "# оценка всех предложений по их схожести с аннотацией\n",
    "def score_for_all_sent(tfidf_text_X, tfidf_text_Y):\n",
    "    score_all_sent = []\n",
    "    for text_index in range(len(tfidf_text_X)):\n",
    "        sentences_X = tfidf_text_X[text_index].toarray()\n",
    "        sentences_Y = tfidf_text_Y[text_index].toarray()\n",
    "        for vec_x in sentences_X:\n",
    "            best_min_score = np.min([score(vec_x, vec_y) for vec_y in sentences_Y])\n",
    "            score_all_sent.append(best_min_score)      \n",
    "    return np.array(score_all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# вспомогательный метод предобработки с формированием пакетов и токенизацией по предложениям\n",
    "def preprocessing_big_text_pack(all_text, punctuation_re):\n",
    "    all_preproc_sent_text = []\n",
    "    count_sent_in_text = []\n",
    "    pack_size = 1000\n",
    "    start_ind = 0  \n",
    "    while start_ind < len(all_text):\n",
    "        pack_list_for_preproc = []\n",
    "        end_ind = start_ind + pack_size\n",
    "        if end_ind > len(all_text):\n",
    "            end_ind = len(all_text)\n",
    "            \n",
    "        # соединение и предобработка\n",
    "        for i in range(start_ind, end_ind):\n",
    "            sentences = nltk.sent_tokenize(all_text[i])\n",
    "            count_sent_in_text.append(len(sentences))\n",
    "            pack_list_for_preproc.append(' bbreakk '.join(sentences))  # между предложениями'bbreakk'\n",
    "        pack_str_for_preproc = ' bbreakk '.join(pack_list_for_preproc)  # между текстами'bbreakk'\n",
    "        pack_lemmas = preprocess(pack_str_for_preproc, punctuation_re)\n",
    "\n",
    "        # разделение на предложения отдельные\n",
    "        preproc_sentences = spliting_list(pack_lemmas, 'bbreakk') \n",
    "        all_preproc_sent_text.extend(preproc_sentences)\n",
    "\n",
    "        start_ind += pack_size\n",
    "    \n",
    "    return np.array(all_preproc_sent_text), np.array(count_sent_in_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделение предложений по текстам, а тексты по наборам X и Y\n",
    "def split_sent_in_text_one_pack(sent, count_X, count_Y):\n",
    "    all_text_X = []\n",
    "    all_text_Y = []\n",
    "    already_taked = 0\n",
    "    for count in count_sent_in_text_X:\n",
    "        all_text_X.append(sent[already_taked:already_taked + count])\n",
    "        already_taked += count\n",
    "    for count in count_sent_in_summ_Y:\n",
    "        all_text_Y.append(sent[already_taked:already_taked + count])\n",
    "        already_taked += count\n",
    "    \n",
    "    return all_text_X, all_text_Y\n",
    "\n",
    "# разделение предложений по текстам\n",
    "def split_sent_in_text_two_pack(sent_X, sent_Y, count_X, count_Y):\n",
    "    all_text_X = []\n",
    "    all_text_Y = []\n",
    "    already_taked = 0\n",
    "    for count in count_sent_in_text_X:\n",
    "        all_text_X.append(sent_X[already_taked:already_taked + count])\n",
    "        already_taked += count\n",
    "    already_taked = 0\n",
    "    for count in count_sent_in_summ_Y:\n",
    "        all_text_Y.append(sent_Y[already_taked:already_taked + count])\n",
    "        already_taked += count\n",
    "    \n",
    "    return all_text_X, all_text_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-reading",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    punctuation_re = re.compile('[^a-zA-Zа-яА-Я0-9]+')\n",
    "    all_text_X_train, all_text_Y_train = unpack(\n",
    "        'C:/Users/Acer/PycharmProjects/ML_NLP_course3/gazeta_jsonl/gazeta_train.jsonl')\n",
    "    all_text_X_val, all_text_Y_val = unpack(\n",
    "        'C:/Users/Acer/PycharmProjects/ML_NLP_course3/gazeta_jsonl/gazeta_val.jsonl')\n",
    "    all_text_X_test, all_text_Y_test = unpack(\n",
    "        'C:/Users/Acer/PycharmProjects/ML_NLP_course3/gazeta_jsonl/gazeta_test.jsonl')\n",
    "    # предобработка тестовых данных, аналогичная валидацинным, не приведена\n",
    "    \n",
    "    all_preproc_sent_X_train, count_sent_in_text_X_train = preprocessing_big_text_pack(all_text_X_train, punctuation_re)\n",
    "    all_preproc_sent_Y_train, count_sent_in_text_Y_train = preprocessing_big_text_pack(all_text_Y_train, punctuation_re)\n",
    "    all_preproc_sent_X_val, count_sent_in_text_X_val = preprocessing_big_text_pack(all_text_X_val, punctuation_re)\n",
    "    all_preproc_sent_Y_val, count_sent_in_text_Y_val = preprocessing_big_text_pack(all_text_Y_val, punctuation_re)\n",
    "    \n",
    "    # надо обработанные тренировочные Х У объединить, чтобы сделать общий мешок слов\n",
    "    all_sent_train = np.concatenate((all_preproc_sent_X_train, all_preproc_sent_Y_train), axis=None)\n",
    "    \n",
    "    tfidf_XY_train, train_vocabulary = tfidf_bag_words_make_vocab(all_sent_train)\n",
    "    tfidf_X_val = tfidf_bag_words(all_preproc_sent_X_val, vocabulary)\n",
    "    tfidf_Y_val = tfidf_bag_words(all_preproc_sent_Y_val, vocabulary)\n",
    "    \n",
    "    # получить список предложений тренировочного набора Х, не разделенных на тексты, для обучения модели \n",
    "    tfidf_no_text_X_train = tfidf_XY_train[:np.sum(count_sent_in_text_X_train)]\n",
    "    \n",
    "    # разделить предложения братно по текстам, а тексты по наборам X и Y\n",
    "    tfidf_text_X_train, tfidf_text_Y_train = split_sent_in_text_one_pack(\n",
    "        tfidf_XY_train, count_sent_in_text_X_train, count_sent_in_text_Y_train\n",
    "    )\n",
    "    tfidf_text_X_val, tfidf_text_Y_val = split_sent_in_text_two_pack(\n",
    "        tfidf_X_val, tfidf_Y_val, count_sent_in_text_X_val, count_sent_in_text_Y_val\n",
    "    )\n",
    "    \n",
    "    # для каждого предложения получить его оценку\n",
    "    score_Y_train = score_for_all_sent(tfidf_text_X_train, tfidf_text_Y_train)\n",
    "    score_Y_val = score_for_all_sent(tfidf_text_X_val, tfidf_text_Y_val)\n",
    "    \n",
    "    # сохранение необходимых данных в файлы .pickle\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
