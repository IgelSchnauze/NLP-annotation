{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "drawn-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nlp = spacy.load('ru2_combined_400ks_96')\n",
    "def_token_sent = nlp.create_pipe('sentencizer')  # add sentence tokenizing\n",
    "nlp.add_pipe(def_token_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "placed-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gazeta_jsonl/gazeta_train.jsonl', \"r\", encoding='utf-8') as f:\n",
    "    json_list = list(f)\n",
    "\n",
    "all_news_text_summ = []\n",
    "for i, str in enumerate(json_list):\n",
    "    if i > 1:\n",
    "        break\n",
    "    one_news = json.loads(str)\n",
    "    del one_news['url']; del one_news['title']; del one_news['date']\n",
    "    all_news_text_summ.append(one_news)\n",
    "    # print(one_news)\n",
    "    # print(isinstance(one_news, dict))  # TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sudden-mention",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_text = all_news_text_summ[0]['text']\n",
    "doc = nlp(new_text)\n",
    "all_preproc_sent_list = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    # lemmatiz + delete stopwords,punct\n",
    "    preproc_sent_list = [token.lemma_ for token in sent if not token.is_stop and not token.is_punct]\n",
    "    all_preproc_sent_list.append(preproc_sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "multiple-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "all_sent_str = [\" \".join(sent_list) for sent_list in all_preproc_sent_list]\n",
    "bag_of_words = vectorizer.fit_transform(all_sent_str)\n",
    "\n",
    "# print(bag_of_words[:1]) # первое предложение\n",
    "# print(bag_of_words[1:2]) # второе предложение (номер будет 0 у всех, если по одному выводить)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tribal-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bag_of_words.toarray())  # столбцы - слова, строки - предложения\n",
    "freq = bag_of_words.toarray().sum(axis=0)  # count frequency every word\n",
    "sort_freq_index = np.argsort(freq)\n",
    "top_words_index = sort_freq_index[-int(len(freq)/10):]  # 10% most freq words\n",
    "# print(freq[sort_freq_index[-int(len(freq)/10):]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "loaded-sullivan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 9]\n"
     ]
    }
   ],
   "source": [
    "# count range for sentences\n",
    "sent_range_list = []\n",
    "for i, sent in enumerate(bag_of_words.toarray()): # take word-index string\n",
    "    use_words_index = np.nonzero(sent)[0]  # [0], because take array from info-tuple\n",
    "    words_count = len(use_words_index)\n",
    "    top_words_count = len(np.intersect1d(top_words_index, use_words_index))\n",
    "    \n",
    "    sent_range = (top_words_count**2)/words_count\n",
    "    sent_range_list.append(sent_range)\n",
    "\n",
    "sort_sent_range = np.argsort(sent_range_list)\n",
    "sent_for_summ_index = np.sort(sort_sent_range[-int(len(sent_range_list)*0.1):])  # 10% most important sentences in right order\n",
    "print(sent_for_summ_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "greenhouse-moscow",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[После октябрьских данных Минэкономразвития вынуждено было изменить оценку, настаивать на $70 млрд означало ожидать серьезного замедления оттока капитала на непонятно каких причинах», — говорит главный экономист BNP Paribas Юлия Цепляева. «, Прогнозируемый Минэкономразвития отток капитала — один из самых высоких за последние 20 лет.]\n"
     ]
    }
   ],
   "source": [
    "# summarize_text = \" \".join([sent for i, sent in enumerate(doc.sents) if any(i==sent_for_summ_index)])\n",
    "summarize_text = [sent for i, sent in enumerate(doc.sents) if any(i == sent_for_summ_index)]\n",
    "print(summarize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "careful-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary rating\n",
    "summary_model = all_news_text_summ[0]['summary']\n",
    "\n",
    "# preprocessing\n",
    "doc_rate = nlp(summary_model)\n",
    "all_summ_preproc_sent_list = []\n",
    "\n",
    "# add generated summary\n",
    "for i, sent_list in enumerate(all_preproc_sent_list):\n",
    "    if any(i == sent_for_summ_index):\n",
    "        all_summ_preproc_sent_list.append(sent_list)\n",
    "# add model summary\n",
    "for sent in doc_rate.sents:\n",
    "    # lemmatiz + delete stopwords,punct\n",
    "    summ_preproc_sent_list = [token.lemma_ for token in sent if not token.is_stop and not token.is_punct]\n",
    "    all_summ_preproc_sent_list.append(summ_preproc_sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "every-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary rating\n",
    "# making bag of words\n",
    "\n",
    "all_summ_sent_str = [\" \".join(sent_list) for sent_list in all_summ_preproc_sent_list]\n",
    "bag_of_words_summ = vectorizer.fit_transform(all_summ_sent_str).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "first-heath",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2701436204546728\n"
     ]
    }
   ],
   "source": [
    "# summary rating\n",
    "\n",
    "# search center vector (vectors = sentence, n-space = number of words in bag)\n",
    "center_gener = np.sum(bag_of_words_summ[:len(sent_for_summ_index)], axis=0)\n",
    "center_model = np.sum(bag_of_words_summ[len(sent_for_summ_index):], axis=0)\n",
    "\n",
    "# count angle between center vectors\n",
    "center_gener_unit = center_gener / np.linalg.norm(center_gener)\n",
    "center_model_unit = center_model / np.linalg.norm(center_model)\n",
    "rating = np.arccos(np.clip(np.dot(center_gener_unit, center_model_unit), -1.0, 1.0))\n",
    "print(rating)\n",
    "# print(np.sqrt(np.sum((center_gener - center_model)**2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
